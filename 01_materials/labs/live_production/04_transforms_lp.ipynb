{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "+ Feature engineering is to transform the data in such a way that the information content is easily exposed to the model.\n",
    "+ This statement can mean many things and highly depends on what exactly is \"the model\".0\n",
    "+ As we have seen, we are using many tools in combination to manipulate data. Thus far, we have encountered pandas, Dask, and sklearn in this course, but there are many more (PySpark, SQL, DAX, M, R, etc.)\n",
    "+ It is important to discuss which tools are the right ones, specifically in the context of data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform using pandas/Dask/SQL or sklearn?\n",
    "\n",
    "+ Depending on the perspective, the answer could be neither, pandas, or sklearn:\n",
    "\n",
    "    - Neither: \n",
    "        * Most join and filtering should be done closer to the source using a database or parquet/Dask operation. \n",
    "        * Map-Reduce and Group-by-Aggregate (\"data warehousing\") operations.\n",
    "        * Indexing and reshuffling.\n",
    "    - Pandas, Dask, or PySpark: \n",
    "        * Renames tasks.\n",
    "        * Use python libraries like pandas, Dask, or pySpark to add contemporaneous feature, time-series manipulation (for example, adding lags), parallel computation (using Dask or pySpark).\n",
    "        * Do not use these libraries for sample-dependent features.\n",
    "    - Use sklearn, pytorch:\n",
    "        * Use python libraries like sklearn or pytorch to add features that are sample-dependent like scaling and normalization, one-hot encoding, tokenization, and vectorization.\n",
    "        * Model-depdenent transformations: PCA, embeddings, iterative/knn imputation, etc.\n",
    "+ Decisions must be guided by optimization criteria (time and resources) while avoiding data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Transforms in sklearn\n",
    "\n",
    "The list below is found in [Scikit's Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules), which also includes convenience interfaces for the classes below.\n",
    "\n",
    "Work with categorical variables:\n",
    "\n",
    "+ `preprocessing.Binarizer(*[, threshold, copy])`: Binarize data (set feature values to 0 or 1) according to a threshold.\n",
    "+ `preprocessing.KBinsDiscretizer([n_bins, ...])`:  Bin continuous data into intervals.\n",
    "+ `preprocessing.LabelBinarizer(*[, neg_label, ...])`: Binarize labels in a one-vs-all fashion.\n",
    "+ `preprocessing.LabelEncoder()`: Encode target labels with value between 0 and n_classes-1.\n",
    "+ `preprocessing.MultiLabelBinarizer(*[, ...])`:  Transform between iterable of iterables and a multilabel format.\n",
    "+ `preprocessing.OneHotEncoder(*[, categories, ...])`: Encode categorical features as a one-hot numeric array.\n",
    "+ `preprocessing.OrdinalEncoder(*[, ...])`: Encode categorical features as an integer array.\n",
    "\n",
    "Scale and normalize:\n",
    "\n",
    "+ `preprocessing.StandardScaler(*[, copy, ...])`: Standardize features by removing the mean and scaling to unit variance.\n",
    "+ `preprocessing.MaxAbsScaler(*[, copy])`: Scale each feature by its maximum absolute value.\n",
    "+ `preprocessing.MinMaxScaler([feature_range, ...])`: Transform features by scaling each feature to a given range.\n",
    "+ `preprocessing.Normalizer([norm, copy])`:  Normalize samples individually to unit norm.\n",
    "+ `preprocessing.RobustScaler(*[, ...])`: Scale features using statistics that are robust to outliers.\n",
    "\n",
    "\n",
    "Nonlinear transforms:\n",
    "\n",
    "+ `preprocessing.FunctionTransformer([func, ...])`: Constructs a transformer from an arbitrary callable.\n",
    "+ `preprocessing.KernelCenterer()`: Center an arbitrary kernel matrix \n",
    "+ `preprocessing.PolynomialFeatures([degree, ...])`: Generate polynomial and interaction features.\n",
    "+ `preprocessing.PowerTransformer([method, ...])`: Apply a power transform featurewise to make data more Gaussian-like.\n",
    "+ `preprocessing.QuantileTransformer(*[, ...])`: Transform features using quantiles information.\n",
    "+ `preprocessing.SplineTransformer([n_knots, ...])`: Generate univariate B-spline bases for features.\n",
    "+ `preprocessing.TargetEncoder([categories, ...])`: Target Encoder for regression and classification targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we doing?\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/column_transform_1.png\" width=\"75%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Objectives\n",
    "\n",
    "Build a pipeline that: \n",
    "\n",
    "+ Add indicators: \n",
    "\n",
    "    - SME indicated that a Debt-to-Ratio > 100% is too high.\n",
    "    - Missing values indicator for `monthly_income` and `num_dependents`.\n",
    "\n",
    "+ Impute missing values, where required.\n",
    "+ Standardize variables.\n",
    "+ Evaluate if a transform (Yeo-Johnson or Box-Cox) of selected variables (debt_ratio, monthly_income, and revolving_unsecured_line_utilization) is beneficial.\n",
    "\n",
    "Feature selection:\n",
    "\n",
    "+ We are looking for informative features: their contribution to prediction is valuable.\n",
    "+ We prefer parsimonious models.\n",
    "+ We want to retain evidence of our work and afford reproducibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "+ For this example, we will use [Give Me Some Credit from Kaggle](https://www.kaggle.com/c/GiveMeSomeCredit/data), a widely refered example. \n",
    "+ To run the examples below, download the data set and extract cs-training.csv to `../05_src/data/credit/`.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Solution\n",
    "\n",
    "+ To get deeper insights into the task, first approach it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation of simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, we obtain a log-loss of about 0.362."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Pipeline\n",
    "\n",
    "+ The pipeline below is more complex:\n",
    "\n",
    "    - Treat selected numericals using [Yeo-Johnson transformation](https://feature-engine.trainindata.com/en/latest/user_guide/transformation/YeoJohnsonTransformer.html).\n",
    "    - Treat other numericals with scaling only.\n",
    "    - Do not treat booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a greater loss of 0.443, therefore the additional feature is not profitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "+ We are currently evaluating two feature engineering procedures using the same classifier. \n",
    "\n",
    "    - However, feature engineering is classifier-dependent: each classifier is a specialized tool to learn a certain type of hypothesis. \n",
    "    - Different classifiers will benefit from different type of engineered features (see, for example, [Khun and Silge's recommendations on TMWR.org](https://www.tmwr.org/pre-proc-table)).\n",
    "\n",
    "+ We are producing data from our experiments.\n",
    "\n",
    "    - The data that we produced is more or less structured: we are using standard performance metrics, for instance.\n",
    "    - Each preprocessing pipeline will be different and may accept different configuration parameters.\n",
    "    - Likewise, classifiers will tend to have different configuration parameters. \n",
    "    \n",
    "+ We modify code to produce experiments:\n",
    "\n",
    "    - Our experiment results will be a function of our algorithm's logic, its implementation (code), and our data.\n",
    "    - Code tracking is doen with Git.\n",
    "    - Data tracking is in development.\n",
    "\n",
    "**It is generally a good idea to use software for experiment tracking once you move out of the Proof of Concept stage.** Some solutions include:\n",
    "\n",
    "- [ML Flow](https://mlflow.org/).\n",
    "- [Weights & Balances](https://wandb.ai/site).\n",
    "- [Sacred](https://sacred.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sacred\n",
    "\n",
    "+ Sacred is a Python package that automates taks related to experiment tracking:\n",
    "\n",
    "    - Keep track of experiment parameters.\n",
    "    - Run experiements using different settings.\n",
    "    - Save configurations for individual experiment runs in files or databases.\n",
    "    - Reproduce results.\n",
    "\n",
    "+ A few features that may be useful:\n",
    "\n",
    "    - Automatically set and store random seeds.\n",
    "    - Keep track of code and artifacts associated with experiment: record the Github repo, hash, and code of the experiment.\n",
    "    - Store experiment run times and system characteristics.\n",
    "    - Work with different backends (\"[Observers](https://sacred.readthedocs.io/en/stable/observers.html)\"): SQL, Mongo, S3, files, Telegram, Slack, and event messges, among others.\n",
    "\n",
    "An important note from [Sacred's documentation](https://sacred.readthedocs.io/en/stable/experiment.html):\n",
    "\n",
    "> By default, Sacred experiments will fail if run in an interactive environment like a REPL or a Jupyter Notebook. This is an intended security measure since in these environments reproducibility cannot be ensured.\n",
    "\n",
    "The safeguard can be relaxed, but generally Production systems do not involve Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments in Sacred\n",
    "\n",
    "\n",
    "+ Experiments in sacred are organized in modules (.py files):\n",
    "\n",
    "    - The main file is called the main *Experiment* file.\n",
    "    - Auxiliary files are called *Ingredients*.\n",
    "\n",
    "+ In the main experiment file, we will instantiate an `Experiment` object:\n",
    "\n",
    "```\n",
    "from sacred import Experiment\n",
    "ex = Experiment(\"Experiment Name\")\n",
    "```\n",
    "\n",
    "+ The `Experiment` object will allow us to use two function decorators:\n",
    "\n",
    "    - `@ex.config`: will decorate the configuration function. All variables declared in this function are observed and made available to all captured functions.\n",
    "    - `@ex.capture`: will decorate one or more captured functions. We can access all variables in `@ex.config`.\n",
    "\n",
    "+ The `Ingredient` objects also have `config` and `capture` decorators that can be used within their own modules. \n",
    "\n",
    "+ `SqlObserver` is the connector between Sacred and a SQL Server. It uses sqlalchemy as an underlying libraries, so URL strings are formatted accordingly.\n",
    "\n",
    "    - SQL Alchemy DB Strings are documented [here](https://docs.sqlalchemy.org/en/20/core/engines.html#database-urls).\n",
    "    - Common DB Strings are:\n",
    "\n",
    "        * Postgres: `postgresql://user:password@localhost/mydatabase`\n",
    "        * SQL Server: `mysql://user:password@localhost/foo`\n",
    "        * SQLite: `sqlite:///foo.db`\n",
    "\n",
    "    - The SQL String for the Docker-based implementation in this repo is in the `../05_src/.env` file, under \"DB_URL\".\n",
    "    - Note that we are passing usernames and passwords through these strings. Although, this may be acceptable for a development environment, usernames and passwords should never be published in Github for production. Use a secrets manager to pass credentials as environment variables in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Experiment\n",
    "\n",
    "Continuing with our example, the following setup will track an experiment to compare the two feature engineering pipelines:\n",
    "\n",
    "+ DB Backend:\n",
    "\n",
    "    - We assume a database backend which can be setup using docker:\n",
    "\n",
    "        * In a terminal, navigate to `./05_src/db/`.\n",
    "        * If the containers are not up, use: `docker compose up -d`\n",
    "        * If you would like to stop the containers use: `docker compose down`.\n",
    "        * Bring the containers down and remove all volumes with: `docker compose down -v` (this will erase all your data in the DB).\n",
    "    \n",
    "    - Notice that all relevant environment variables for the DB server are in `./05_src/db/env`.\n",
    "\n",
    "+ The Experiment file:\n",
    "\n",
    "    - The main file for this experiment is `./05_src/credit_experiment_nb.py`\n",
    "    - Lines 25-26 instantiate the experiment and import all the ingredients: `data_ingredient`, `preproc_ingredient`, and `db_ingredient`.\n",
    "    - We use our standard logger (line 24), and we share it with the experiment tracker (line 28).\n",
    "    - Configuration of the experiment is defined in lines 31-35:\n",
    "\n",
    "        * Line 31 has the decorator `@ex.config`.\n",
    "        * We define the preproc_pipe, number of folds, and scoring metrics as the experiment's configuration. \n",
    "        \n",
    "    - Captured functions:\n",
    "        \n",
    "        * Notice that the function `get_pipe()` in line 40 requireds `preproc_pipe`. When the function is called, sacred will ensure that the relevant value of `preproc_pipe` is used in place of this input parameter.\n",
    "\n",
    "    - Main function:\n",
    "\n",
    "        * The main function is identified by `@ex.automain` or `@ex.main`.\n",
    "        * As well, the lines 79-80 add commands to modify experiments from [the CLI](https://sacred.readthedocs.io/en/stable/command_line.html).\n",
    "        * This is the function that is run from the CLI: `python credit_experiment_nb.py`\n",
    "    \n",
    "\n",
    "+ Ingredients:\n",
    "\n",
    "    - We create other modules to organize our ideas and code.\n",
    "    - The preproc ingredient (`./05_src/credit_preproc_ingredient.py`) encapsulates the preprocessing logic: selecting the right pipeline, for example.\n",
    "    - The data ingredient (`./05_src/credit_data_ingredient.py`) loads and performs the panda-based manipulations. \n",
    "    - The db ingredient (`./05_src/credit_db_ingredient.py`) keeps all functions related to db interactions and authentication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ensuring your docker containers are up, run the experiment with \n",
    "\n",
    "```\n",
    "cd src/\n",
    "python credit_experiment.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the experiment, take a look at your database:\n",
    "\n",
    "+ Navigate to [http://localhost:5051](http://localhost:5051).\n",
    "+ Login using posgres/HumanAfterAll.\n",
    "+ Connect to the db (its name is db in the local network) and query the runs table and model_cv_results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
