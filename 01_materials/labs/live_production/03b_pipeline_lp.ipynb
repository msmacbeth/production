{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An initial training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "+ Previously, we produced a features data set.\n",
    "+ Most times, one or more [preprocessing steps](https://scikit-learn.org/stable/modules/preprocessing.html#) steps will be applied to data.\n",
    "+ The most practical way to apply them is by arranging them in `Pipeline` objects, wchich are sequential transformations applied to data. \n",
    "+ It is convenient for us to label these transformations and there is a standard way of doing so.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "+ Transformations are classes that implement `fit` and `transform` methods.\n",
    "\n",
    "### StandardScaler\n",
    "\n",
    "+ For example, transform a numerical variable by standardizing it.\n",
    "- Standardization is removing the mean value of the feature and scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "$$\n",
    "z = \\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "\n",
    "+  Using [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), one can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler object\n",
    "\n",
    "\n",
    "\n",
    "# Fit the StandardScaler object with the returns data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the returns data using the fitted scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  OneHotEncoder\n",
    "\n",
    "+ Categorical features can be encoded as numerical values using `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Use [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to encode a categorical variable as numerical.\n",
    "+ Important parameters:\n",
    "\n",
    "    - `categories` allows you to specify the categories to work with.\n",
    "    - `drop`: we can drop the `'first'` value (dummy encoding) or `'if_binary'`, a convenience setting for binary values.\n",
    "    - `handle_unknown` allows three options, `'error'`, `'ignore'`, and `'infrequent_if_exist'`, depending on what we want to do with new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "+ It is impractical and costly to manipulate data \"by hand\". \n",
    "+ To manage data preprocessing steps within the cross-validation process use `Pipeline` objects.\n",
    "+ A [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) object allows us to sequentially apply transformation steps and, if required, a predictor.\n",
    "+ `Pipeline` objects compose transforms, i.e., classes that implement `transform` and `fit` methods.\n",
    "+ The purpose of `Pipeline` objects is to ensemble transforms and predictors to be used in cross-validation.\n",
    "+ A `Pipeline` is defined by a list of tuples.\n",
    "+ Each tuple is composed of `(\"name\", <ColumnTransformer>)`, the name of the step and the `<ColumnTransformer>` function of our chosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The model does not show great performance, but the pipeline shows results. \n",
    "+ Below, we expand the pipeline to include more variables, and further we will work with more robust model selection pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColumnTransformer\n",
    "\n",
    "+ Use [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to apply transformers to specific columns of a DataFrame.\n",
    "+ In this case, we will scale numeric variables and apply one-hot encoding to categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "The model selection process is an iterative process in which :\n",
    "\n",
    "+ Select schema and load data.\n",
    "+ Define a pipeline and its (hyper) parameters.\n",
    "\n",
    "    - Use ColumnTransformers to transform numeric and cateogrical variables.\n",
    "    - Hyperparameters can be defined independently of code. \n",
    "\n",
    "+ Implement a splitting strategy. \n",
    "\n",
    "    - Use [cross_validate]() to select several metrics and operational details.\n",
    "\n",
    "+ Measure performance.\n",
    "\n",
    "    - [Select metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "\n",
    "+ Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Testing Split\n",
    "\n",
    "+ The first spliting strategy is to use a training, validation, and test set.\n",
    "+ Training set will be used to fit the model.\n",
    "+ Validation set is used to evaluate hyperparameter choice.\n",
    "+ Testing set is used to evaluate performance on data the model has not yet seen.\n",
    "+ In this case we want to compare two models: \n",
    "\n",
    "    - Decision Tree with 3 minumum samples per leaf.\n",
    "    - Decision Tree with 10 minimum samples per leaf.\n",
    "\n",
    "![](../images/03b_train_validate_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters in pipeline steps\n",
    "\n",
    "+ One can obtain the parameters of a pipeline with `pipe.get_params()`.\n",
    "+ We can set any parameter of a pipeline with `pipe.set_parames(**kwargs)`. \n",
    "+ The input `**kwargs` is a dictionary of the params to be modified. Params of the steps are labeled with the name of the step followed by `__` and the name of the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ There are a few steps that we will repeat: \n",
    "\n",
    "    - Fit the candidate model on training data.\n",
    "    - Predict on training and test data.\n",
    "    - Compute training and test performance metrics.\n",
    "    - Return.\n",
    "\n",
    "+ We encapsulate this procedure in a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "\n",
    "\n",
    "# Split the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hyperparameter configuration 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hyperparameter configuration 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "+ Cross-validation is a resampling method.\n",
    "+ It is an iterative method applied to training data.\n",
    "+ Training data is divided into folds.\n",
    "+ Each fold is used once as a validation set and the rest of the folds are used for training.\n",
    "+ Test data is used for final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Scikit's Documentation ](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance), the diagram below shows the data divisions and folds during the cross-validation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/03b_grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two functions that can be used for [calculating cross-validation performance scores](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance): `cross_val_score()` and `cross_validate()`. The first function, [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score), is a convenience function to get quick perfromance calculations. We will discuss `cross_validate()` as it offers advantages over `cross_val_score()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining metrics\n",
    "\n",
    "+ Use [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) to measure one or more performance metrics and operational details.\n",
    "+ There are two advantages of using this function. From [Scikit's documentation](https://scikit-learn.org/stable/modules/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation):\n",
    "\n",
    ">- It allows specifying multiple metrics for evaluation.\n",
    ">- It returns a dict containing fit-times, score-times (and optionally training scores, fitted estimators, train-test split indices) in addition to the test score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DataFrame form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Performance\n",
    "\n",
    "+ Notice that in order to acquire information about our model and continue development, we are spending resources: time, electricity, equipment use, etc. As well, we are generating data and binary objects that implement our models (fitted `Pipeline` objects, for example).\n",
    "+ For certain applications, operating performance (latency or `'score_time'`) may be as important or more important than predictive performance metrics. \n",
    "+ Every experiment throws important information and we can log them, as well as run them systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
