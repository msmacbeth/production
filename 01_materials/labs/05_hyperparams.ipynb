{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we doing?\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Construct a cross-validation pipeline.\n",
    "+ Use cross-validation to evaluate different hyperparameter performance.\n",
    "+ Perform grid search for systemic evaluation.\n",
    "+ Store and manage results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "The diagram below, taken from Scikit Learn's documentation, shows the procedure that we will follow:\n",
    "\n",
    "![](./images/05_grid_search_workflow.png)\n",
    "\n",
    "\n",
    "+ System requriements:\n",
    "    \n",
    "    - Automation: the system should operate automatically with the least amount of supervision. \n",
    "    - Replicability: changes to code and (arguably) data should be logged and controled. Randomness should also be controlled (random seeds, etc.)\n",
    "    - Persistence: persist results for later analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Hyperparameter?\n",
    "\n",
    "+ Generally speaking, hyperparameters are parameters that control the learning process: regularization weights, learning rate, entropy/gini metrics, etc. \n",
    "+ Hyperparameters will drive the behaviour and performance of a model. Model selection is intimately related with hyperparameter tuning. \n",
    "+ Selection critieria are based on performance evaluation and, to get better performance estimates, we use cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the Hyperparameter Grid\n",
    "\n",
    "+ To address the automation requirement, we could use `GridSearchCV()`, which is a self-contained function for performing a Grid Search over a hyperparameter space.\n",
    "+ To \"Search the Hyperparameter Grid\" exhaustively means that we will consider all possible combination of hyperparameter values in the search space and evaluate the model using those hyperparams. For example, if we have two parameters that we are exploring, kernel (takes values \"rbf\" and \"poly\") and C (takes values 1.0 and 0.5), then this grid would be the combinations:\n",
    "\n",
    "    + (rbf, 1.0)\n",
    "    + (rbf, 0.5)\n",
    "    + (poly, 1.0)\n",
    "    + (poly, 0.5)\n",
    "\n",
    "+ Under each combination, we perform CV and evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "We start with [Give me some credit](https://www.kaggle.com/c/GiveMeSomeCredit) data that we used in the previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv \n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "ft_path = os.getenv(\"CREDIT_DATA\")\n",
    "df_raw = pd.read_csv(ft_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.drop(columns = [\"Unnamed: 0\"]).rename(\n",
    "    columns = {\n",
    "        'SeriousDlqin2yrs': 'delinquency',\n",
    "        'RevolvingUtilizationOfUnsecuredLines': 'revolving_unsecured_line_utilization', \n",
    "        'age': 'age',\n",
    "        'NumberOfTime30-59DaysPastDueNotWorse': 'num_30_59_days_late', \n",
    "        'DebtRatio': 'debt_ratio', \n",
    "        'MonthlyIncome': 'monthly_income',\n",
    "        'NumberOfOpenCreditLinesAndLoans': 'num_open_credit_loans', \n",
    "        'NumberOfTimes90DaysLate':  'num_90_days_late',\n",
    "        'NumberRealEstateLoansOrLines': 'num_real_estate_loans', \n",
    "        'NumberOfTime60-89DaysPastDueNotWorse': 'num_60_89_days_late',\n",
    "        'NumberOfDependents': 'num_dependents'\n",
    "    }\n",
    ").assign(\n",
    "    high_debt_ratio = lambda x: (x['debt_ratio'] > 1)*1,\n",
    "    missing_monthly_income = lambda x: x['monthly_income'].isna()*1,\n",
    "    missing_num_dependents = lambda x: x['num_dependents'].isna()*1, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a simple pipeline composed of:\n",
    "\n",
    "+ Preprocessing steps.\n",
    "+ Logistic Regression classifier.\n",
    "\n",
    "We will explore the hyperparameter sapce by evaluating different regularization strategies and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = ['revolving_unsecured_line_utilization', 'age',\n",
    "       'num_30_59_days_late', 'debt_ratio', 'monthly_income',\n",
    "       'num_open_credit_loans', 'num_90_days_late', 'num_real_estate_loans',\n",
    "       'num_60_89_days_late', 'num_dependents', \n",
    "       # Although expressed as numbers, these columns are boolean:\n",
    "       # 'high_debt_ratio',\n",
    "       # 'missing_monthly_income', \n",
    "       # 'missing_num_dependents' \n",
    "       ]\n",
    "\n",
    "pipe_num_simple = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('standardizer', StandardScaler())\n",
    "])\n",
    "\n",
    "ctransform_simple= ColumnTransformer([\n",
    "    ('numeric_simple', pipe_num_simple, num_cols),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('preprocess', ctransform_simple),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "pipe_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the parameters of the pipeline with `.get_params()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Splitting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'delinquency')\n",
    "Y = df['delinquency']\n",
    "\n",
    "scoring = ['neg_log_loss', 'roc_auc', 'f1', 'accuracy', 'precision', 'recall']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the Grid Search we need to define a parameter grid:\n",
    "\n",
    "- A parameter grid defines all of the combinations of parameters that we need to explore.\n",
    "- The function `GridSearchCV()` performs an exhaustive search of parameter combinations.\n",
    "- The parameter grid is defined as a dictionary of lists:\n",
    "\n",
    "    * Each entry's key is the name of the parameter.\n",
    "    * Each entry's value is the list of values that we would like to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'clf__C': [0.01, 0.5, 1.0],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__solver': ['liblinear'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key inputs to [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) are:\n",
    "\n",
    "+ `estimator`: the pipeline or classifier that we are tuning.\n",
    "+ `param_grid`: the parameter grid defined as a dictionary of lists described above.\n",
    "+ `n_jobs`: settings for parallel computation.\n",
    "+ `refit`: options for refitting the model using the best-performing configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv = GridSearchCV(\n",
    "    estimator=pipe_lr, \n",
    "    param_grid=param_grid, \n",
    "    scoring = scoring, \n",
    "    cv = 5,\n",
    "    refit = \"neg_log_loss\")\n",
    "grid_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the cross-validation results using the property `.cv_results_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = grid_cv.cv_results_\n",
    "res = pd.DataFrame(res)\n",
    "res.columns\n",
    "\n",
    "res[['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
    "       'param_clf__C', 'param_clf__penalty', 'param_clf__solver', 'params',\n",
    "       'mean_test_neg_log_loss',\n",
    "       'std_test_neg_log_loss', 'rank_test_neg_log_loss']].sort_values('rank_test_neg_log_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the best-performing configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing classifier (pipeline) trained on the complete training set is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking GridSearchCV Experiments\n",
    "\n",
    "+ We can expand our infrastructure for hyperparameter tuning across various models.\n",
    "+ The plan:\n",
    "\n",
    "    - Create a model ingredient to obtain the classifier object.\n",
    "    - Create experiment param grids in json files to organize our parameter grids.\n",
    "    - Schedule the experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Design\n",
    "\n",
    "<div>\n",
    "<img src=./images/05_experiment_setup.png width=\"75%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the code in `./05_src/credit_experiment.py` and `./05_src/credit_model_ingredient.py`:\n",
    "\n",
    "+ `credit_model_ingredient.py` implements a function that returns a model given a string. This way, we can parametrize models in the experiment.\n",
    "+ `credit_experiment.py` is modularized version of our previous file, `credit_experiment_nb.py` which only worked with Naive Bayes classifier.\n",
    "+ The experiment is now further *modularized*: there are ingredients for most components and it can be broken down even more depending on the evolution of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiments from the Command Line\n",
    "\n",
    "Access the experiment through the [Command Line Interface](https://sacred.readthedocs.io/en/stable/command_line.html).\n",
    "\n",
    "```\n",
    "cd src  # if required\n",
    "python credit_experiment.py\n",
    "```\n",
    "\n",
    "We can also change the parameters of the experiment. For instance, using the same code, we can run an experiment with a logistic regression classifier using a basic (not power) preprocessing pipeline:\n",
    "\n",
    "```\n",
    "python .\\credit_experiment.py with 'preprocessing=\"basic\"' 'model=\"LogisticRegression\"'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few Notes About Sacred\n",
    "\n",
    "+ Sacred is a powerful tool, but it is only the beginning.  \n",
    "+ Sacred is useful in keeping track of experiments within a limited scope: it is not a project management tool.\n",
    "+ It works well in SQL environments, but handling hyperparameters can be painful.\n",
    "+ The natural backend is MongoDB, however not all workplaces have running instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Schema\n",
    "\n",
    "The database schema implemented by sacred is shown below. The schema is a useful representation of the code and setup of an experiment. The package offers a [metrics API](https://sacred.readthedocs.io/en/stable/collected_information.html#metrics-api), but we have decided to extend the framework with a few ad-hoc tables with performance metrics. \n",
    "\n",
    "The database backend is a database like any other: you can query it with Python, R, or PowerBI.\n",
    "\n",
    "+ Server is located in localhost port 5432.\n",
    "+ User and password are in the .env file in `./05_src/db/`.\n",
    "\n",
    "<div>\n",
    "<img src=./images/05_sacred_sql_schema.png width=\"40%\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
